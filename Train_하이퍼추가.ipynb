{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train_H.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b5a36a64a9124898a616669e97a298b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6edfb32b17cd4d929632129dcc4a16e7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_666cfc98529e48469ac11da910867702","IPY_MODEL_67e6cb074b9243a48a4ba8a78b0c723b"]}},"6edfb32b17cd4d929632129dcc4a16e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"666cfc98529e48469ac11da910867702":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_43ee0eb317e74a34aef1ece938d218a8","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":995526,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":995526,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef104ea390904b17950ca8b1ac7b2286"}},"67e6cb074b9243a48a4ba8a78b0c723b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a67454473b414bdf84c37a654a306ecb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 996k/996k [00:01&lt;00:00, 503kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_44c573cfa58c425d9bc8bbfd026e9e07"}},"43ee0eb317e74a34aef1ece938d218a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ef104ea390904b17950ca8b1ac7b2286":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a67454473b414bdf84c37a654a306ecb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"44c573cfa58c425d9bc8bbfd026e9e07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5e18ef07611f473a88654724a6f58187":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1213f763f434420fbaca3b20632424a1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ad5e8f038e0d430a8dc96c635e809831","IPY_MODEL_2a4edb1dceec49feb17e420ca71e65eb"]}},"1213f763f434420fbaca3b20632424a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad5e8f038e0d430a8dc96c635e809831":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_98b7fde0aa0444c48b7db85c64b6d9b8","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1961828,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1961828,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e8fe0971b9ef41b2bfa4224957283478"}},"2a4edb1dceec49feb17e420ca71e65eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e70bee6efed2472fa694ce85e4f27989","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.96M/1.96M [00:00&lt;00:00, 2.76MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d36b4b2890604b939dce79e89ae72f78"}},"98b7fde0aa0444c48b7db85c64b6d9b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e8fe0971b9ef41b2bfa4224957283478":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e70bee6efed2472fa694ce85e4f27989":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d36b4b2890604b939dce79e89ae72f78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"38596b11ee514212a2ab1741ce974ded":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bd1ad3beee0f4a21921b42f6662cc0b9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4fb370a8ae7d485dbe035600ac4a0a42","IPY_MODEL_b4b1cc60e5704ebd8f20f910945e7b6f"]}},"bd1ad3beee0f4a21921b42f6662cc0b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4fb370a8ae7d485dbe035600ac4a0a42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8b462409f53746749b7ec53e6b75d06e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":625,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":625,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c7266cab9f804377bc89edffca607818"}},"b4b1cc60e5704ebd8f20f910945e7b6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_16847499b0444d089548f6291eba1a4e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 625/625 [00:12&lt;00:00, 48.1B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_413e4c938b8b4547855d6df718dc79a0"}},"8b462409f53746749b7ec53e6b75d06e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c7266cab9f804377bc89edffca607818":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"16847499b0444d089548f6291eba1a4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"413e4c938b8b4547855d6df718dc79a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"652a20f9cad34d359b7af0bec2aa830f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_470123da64f64a77aa16cb0323edbeb4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1ab4498b061a484e84c5b445daa48191","IPY_MODEL_fbea81b56b684c8a808bd0ed7fb844f9"]}},"470123da64f64a77aa16cb0323edbeb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1ab4498b061a484e84c5b445daa48191":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a25a1773d6f542baaff81fd40d2f09df","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":714314041,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":714314041,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5c786f802036446ab2d784d92d3bfba0"}},"fbea81b56b684c8a808bd0ed7fb844f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_90275dc0a2894c58b432bf6a7d146fde","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 714M/714M [00:11&lt;00:00, 61.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_420b9ca6dfde4eb983095e318c4ce95d"}},"a25a1773d6f542baaff81fd40d2f09df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5c786f802036446ab2d784d92d3bfba0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90275dc0a2894c58b432bf6a7d146fde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"420b9ca6dfde4eb983095e318c4ce95d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"CFLn7sRsjjbx"},"source":["# DACON \"AI야, 진짜 뉴스를 찾아줘!\" - 북극506번지  \r\n","  \r\n","주 언어 : Pytorch"]},{"cell_type":"markdown","metadata":{"id":"3zPWxxuLjwvW"},"source":["## Install Libraries"]},{"cell_type":"code","metadata":{"id":"C8Mb6s66QyX_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609255482908,"user_tz":-540,"elapsed":8263,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}},"outputId":"68266fa6-2304-4c8d-80ad-6eb1f6da2183"},"source":["# Hugging Face의 Transformers\r\n","!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 14.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 41.9MB/s \n","\u001b[?25hCollecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 44.6MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=2ac1eee27ccf1d14427edc0b1e57829893a636d769362f38330900980880c8f3\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JAUdGPBbkQzL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609255497670,"user_tz":-540,"elapsed":23017,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}},"outputId":"850ff05d-88ee-4f29-ca32-dbabf63b13a2"},"source":["from google.colab import drive\r\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TlNdCDBvkGRu","executionInfo":{"status":"ok","timestamp":1609255502337,"user_tz":-540,"elapsed":27682,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}}},"source":["# 데이터 전처리를 위한 라이브러리\r\n","import numpy as np\r\n","import pandas as pd\r\n","import warnings\r\n","import os, sys\r\n","import random\r\n","import time\r\n","import datetime\r\n","%matplotlib inline\r\n","warnings.filterwarnings('ignore')\r\n","pd.set_option('display.max_row', 500)\r\n","\r\n","# 모델링을 위한 라이브러리\r\n","from sklearn.model_selection import train_test_split\r\n","import tensorflow as tf\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","import torch\r\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","from transformers import BertTokenizer, BertTokenizerFast\r\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n","from transformers import get_linear_schedule_with_warmup"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eg4f52uMt9yu"},"source":["## Device Setting"]},{"cell_type":"code","metadata":{"id":"mDVxIQFnuACk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609255502338,"user_tz":-540,"elapsed":27677,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}},"outputId":"0ae55343-fbe5-44e0-fef6-2d5c8ef187cb"},"source":["# 디바이스 설정\r\n","if torch.cuda.is_available():    \r\n","    device = torch.device(\"cuda\")\r\n","    print('사용 가능한 %d개의 GPU가 존재합니다.' % torch.cuda.device_count())\r\n","    print('{} GPU를 사용할 예정입니다.'.format(torch.cuda.get_device_name(0)))\r\n","else:\r\n","    device = torch.device(\"cpu\")\r\n","    print('사용 가능한 GPU가 존재하지 않으므로, CPU를 사용할 예정입니다.')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["사용 가능한 1개의 GPU가 존재합니다.\n","Tesla V100-SXM2-16GB GPU를 사용할 예정입니다.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4Go55IVVt8kX"},"source":["## Data Loading"]},{"cell_type":"code","metadata":{"id":"v7sTIz4Vkd_b","executionInfo":{"status":"ok","timestamp":1609255504723,"user_tz":-540,"elapsed":30060,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}}},"source":["# 학습 데이터 로드\r\n","train = pd.read_csv('/content/gdrive/My Drive/NH공모전/data/news_train.csv', encoding = 'utf-8-sig')"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EBes5HXYkhBM"},"source":["## Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"o4ZrjJnvn51S"},"source":["### Ord  \r\n","  \r\n","뉴스 기사 내에서 문장의 위치를 나타내는 변수.  \r\n","> `ordp_s` 생성  \r\n","  = (ord / max_ord) * 100. 뉴스 기사 내에서 문장의 상대적인 위치를 나타내는 변수."]},{"cell_type":"code","metadata":{"id":"Jl6RSZBtoJvH","executionInfo":{"status":"ok","timestamp":1609255504724,"user_tz":-540,"elapsed":30059,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}}},"source":["max_ord = train.groupby('n_id').max(ord).ord\r\n","train = pd.merge(train, max_ord, on='n_id')\r\n","train['ordp'] = train.ord_x/train.ord_y\r\n","train['ordp_s'] = (train.ordp*100).astype(int).astype(str)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OZ5fH-uZn4nU"},"source":["### Content  \r\n","  \r\n","뉴스 기사 내용을 의미하는 변수.  \r\n","특수문자, 조사 등은 제거하지 않음.  \r\n","\r\n","### Title  \r\n","\r\n","뉴스 기사 제목을 의미하는 변수.\r\n","특수문자, 조사 등은 제거하지 않음. "]},{"cell_type":"markdown","metadata":{"id":"Q6-F15MarZmv"},"source":["### Dataset \r\n","  \r\n","> Input = `content` + '  ' + `ordp_s` + '  ' + `title`"]},{"cell_type":"code","metadata":{"id":"AVHUGkYtkoTJ","colab":{"base_uri":"https://localhost:8080/","height":190,"referenced_widgets":["b5a36a64a9124898a616669e97a298b0","6edfb32b17cd4d929632129dcc4a16e7","666cfc98529e48469ac11da910867702","67e6cb074b9243a48a4ba8a78b0c723b","43ee0eb317e74a34aef1ece938d218a8","ef104ea390904b17950ca8b1ac7b2286","a67454473b414bdf84c37a654a306ecb","44c573cfa58c425d9bc8bbfd026e9e07","5e18ef07611f473a88654724a6f58187","1213f763f434420fbaca3b20632424a1","ad5e8f038e0d430a8dc96c635e809831","2a4edb1dceec49feb17e420ca71e65eb","98b7fde0aa0444c48b7db85c64b6d9b8","e8fe0971b9ef41b2bfa4224957283478","e70bee6efed2472fa694ce85e4f27989","d36b4b2890604b939dce79e89ae72f78"]},"executionInfo":{"status":"ok","timestamp":1609255533391,"user_tz":-540,"elapsed":58722,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}},"outputId":"1cab23c0-5d32-41c0-aa61-2333df9a474a"},"source":["# Input sentences 생성\r\n","sentences = train['content'] + ' ' + train['ordp_s'] + ' ' + train['title']\r\n","\r\n","# Input sentences를 BERT의 입력 형식에 맞게 변환\r\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\r\n","\r\n","# Label 추출\r\n","labels = train['info'].values\r\n","\r\n","# BERT의 토크나이저로 문장을 토큰으로 분리\r\n","# bert-base-multilingual-cased pretrained tokenizer 사용\r\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\r\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\r\n","\r\n","print (sentences[0])\r\n","print (tokenized_texts[0])"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5a36a64a9124898a616669e97a298b0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e18ef07611f473a88654724a6f58187","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961828.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"},{"output_type":"stream","text":["\n","[CLS] [이데일리 MARKETPOINT]15:32 현재 코스닥 기관 678억 순매도 25 [마감]코스닥 기관 678억 순매도 [SEP]\n","['[CLS]', '[', '이', '##데', '##일', '##리', 'MA', '##R', '##K', '##ET', '##PO', '##IN', '##T', ']', '15', ':', '32', '현재', '코', '##스', '##닥', '기', '##관', '678', '##억', '순', '##매', '##도', '25', '[', '마', '##감', ']', '코', '##스', '##닥', '기', '##관', '678', '##억', '순', '##매', '##도', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"36CvG0tCsLsv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609255538930,"user_tz":-540,"elapsed":64254,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}},"outputId":"16dce0ae-3fac-46f5-fb45-8e2dd92e83b7"},"source":["# 입력 토큰의 최대 시퀀스 길이\r\n","MAX_LEN = 100\r\n","\r\n","# 토큰을 숫자 인덱스로 변환\r\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\r\n","\r\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\r\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\r\n","\r\n","input_ids[0]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   101,    164,   9638,  28911,  18392,  12692,  27277,  11273,\n","        11733,  52338,  93520,  27128,  11090,    166,  10208,    131,\n","        10842,  26565,   9812,  12605, 118770,   8932,  20595,  69015,\n","        91837,   9462, 100372,  12092,  10258,    164,   9246, 105197,\n","          166,   9812,  12605, 118770,   8932,  20595,  69015,  91837,\n","         9462, 100372,  12092,    102,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"wLYzBv3usZih","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609255546588,"user_tz":-540,"elapsed":71908,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}},"outputId":"2ddc7305-fe46-48c7-e7c0-6e9b13254bd5"},"source":["# 어텐션 마스크 초기화\r\n","attention_masks = []\r\n","\r\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\r\n","# 어텐션 마스크 0인 부분은 계산이 이루어지지 않으므로 속도 향상 가능\r\n","for seq in input_ids:\r\n","    seq_mask = [float(i>0) for i in seq]\r\n","    attention_masks.append(seq_mask)\r\n","\r\n","print(attention_masks[0])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FuyrVry0tWAJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609255547725,"user_tz":-540,"elapsed":73040,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}},"outputId":"180c72dc-8d1c-477d-ae7a-3b9fb5cbbe4e"},"source":["# 데이터를 훈련셋과 검증셋으로 분리\r\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\r\n","                                                                                    labels, \r\n","                                                                                    random_state=2020, \r\n","                                                                                    test_size=0.2)\r\n","\r\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\r\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, \r\n","                                                       input_ids,\r\n","                                                       random_state=2020, \r\n","                                                       test_size=0.2)\r\n","\r\n","# 데이터를 텐서로 변환\r\n","train_inputs = torch.tensor(train_inputs)\r\n","train_labels = torch.tensor(train_labels)\r\n","train_masks = torch.tensor(train_masks)\r\n","validation_inputs = torch.tensor(validation_inputs)\r\n","validation_labels = torch.tensor(validation_labels)\r\n","validation_masks = torch.tensor(validation_masks)\t\t\t\t\r\n","\r\n","print(train_inputs[0])\r\n","print(train_labels[0])\r\n","print(train_masks[0])\r\n","print(validation_inputs[0])\r\n","print(validation_labels[0])\r\n","print(validation_masks[0])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tensor([   101,   9689,  21155,  14040,  83685,   9566,  32159,  25805,   9513,\n","          9566,  14423,  37568,   9684,  68055,    119,   9284,  37824,  17322,\n","           110,  66982,    119,  11978,   2650,    117,   9309,  37388,  43022,\n","         33188,  21928,   9736,  21928,  10530,    100,   9356,  15891,   9098,\n","        106249,  27654,   9657,  40419,  88236,   8987,  20626,  18471,   9067,\n","         20479,    100,    102,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0])\n","tensor(1)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n","tensor([   101,  23607,   3642,   9414,  30134,  12310,    107,   9670,  14646,\n","         16605, 119254,    107,  99405,   9460, 119437,  16323,  98151,  10150,\n","          8933,  37568,  28000,  21789,    119,    119,  10296,  48556,  12310,\n","         26784, 119398,  11287,    113, 103542,  11373,  28847,    114,    117,\n","         11525,  16323,   9487,  11664,  11287,    102,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0])\n","tensor(1)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R2m5MRzFtoql","executionInfo":{"status":"ok","timestamp":1609255547725,"user_tz":-540,"elapsed":73038,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}}},"source":["# 배치 사이즈\r\n","batch_size = 32\r\n","\r\n","# DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\r\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\r\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\r\n","train_sampler = RandomSampler(train_data)\r\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n","\r\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\r\n","validation_sampler = SequentialSampler(validation_data)\r\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lSse_3qvt1RE"},"source":["## Modeling"]},{"cell_type":"code","metadata":{"id":"z6EzMst1ufo_","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["38596b11ee514212a2ab1741ce974ded","bd1ad3beee0f4a21921b42f6662cc0b9","4fb370a8ae7d485dbe035600ac4a0a42","b4b1cc60e5704ebd8f20f910945e7b6f","8b462409f53746749b7ec53e6b75d06e","c7266cab9f804377bc89edffca607818","16847499b0444d089548f6291eba1a4e","413e4c938b8b4547855d6df718dc79a0","652a20f9cad34d359b7af0bec2aa830f","470123da64f64a77aa16cb0323edbeb4","1ab4498b061a484e84c5b445daa48191","fbea81b56b684c8a808bd0ed7fb844f9","a25a1773d6f542baaff81fd40d2f09df","5c786f802036446ab2d784d92d3bfba0","90275dc0a2894c58b432bf6a7d146fde","420b9ca6dfde4eb983095e318c4ce95d"]},"executionInfo":{"status":"ok","timestamp":1609255573894,"user_tz":-540,"elapsed":99202,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}},"outputId":"faab3f06-0532-4f69-ba14-4594c4b04e4a"},"source":["# 분류를 위한 BERT 모델 생성\r\n","# bert-base-multilingual-cased model 사용\r\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\r\n","model.cuda()"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38596b11ee514212a2ab1741ce974ded","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"652a20f9cad34d359b7af0bec2aa830f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"pgJb4QvtuoVc","executionInfo":{"status":"ok","timestamp":1609255573894,"user_tz":-540,"elapsed":99200,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}}},"source":["# 옵티마이저 설정\r\n","optimizer = AdamW(model.parameters(),\r\n","                  lr = 2e-5, # 학습률\r\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\r\n","                )\r\n","\r\n","# 에폭수\r\n","epochs = 3\r\n","\r\n","# 총 훈련 스텝 = 배치반복 횟수 * 에폭\r\n","total_steps = len(train_dataloader) * epochs\r\n","\r\n","# 학습률을 조금씩 변화시키는 스케줄러 생성 (선형적으로 감소)\r\n","scheduler = get_linear_schedule_with_warmup(optimizer, \r\n","                                            num_warmup_steps = 0,\r\n","                                            num_training_steps = total_steps)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeBR0Bq0xDHN","executionInfo":{"status":"ok","timestamp":1609255573895,"user_tz":-540,"elapsed":99199,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}}},"source":["# 정확도 계산 함수\r\n","def calculate_accuracy(preds, labels):\r\n","    \r\n","    pred_flat = np.argmax(preds, axis=1).flatten()\r\n","    labels_flat = labels.flatten()\r\n","\r\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n","\r\n","# 시간 표시 함수\r\n","def format_time(elapsed):\r\n","\r\n","    # 반올림\r\n","    elapsed_rounded = int(round((elapsed)))\r\n","    \r\n","    # hh:mm:ss으로 형태 변경\r\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"wrofsfwnxRYZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609257480637,"user_tz":-540,"elapsed":2005935,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}},"outputId":"0d5cfba4-99fe-4846-b381-c9c6608b4f72"},"source":["# 랜덤 시드 고정\r\n","seed_val = 42\r\n","random.seed(seed_val)\r\n","np.random.seed(seed_val)\r\n","torch.manual_seed(seed_val)\r\n","torch.cuda.manual_seed_all(seed_val)\r\n","\r\n","# 그래디언트 초기화\r\n","model.zero_grad()\r\n","\r\n","# 에폭만큼 반복\r\n","for epoch_i in range(0, epochs):\r\n","    \r\n","    # ========================================\r\n","    #               Training\r\n","    # ========================================\r\n","    \r\n","    print(\"\")\r\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n","    print('Training...')\r\n","\r\n","    # 시작 시간 설정\r\n","    t0 = time.time()\r\n","\r\n","    # 로스 초기화\r\n","    total_loss = 0\r\n","\r\n","    # 훈련모드로 변경\r\n","    model.train()\r\n","        \r\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\r\n","    for step, batch in enumerate(train_dataloader):\r\n","        # 경과 정보 표시\r\n","        if step % 500 == 0 and not step == 0:\r\n","            elapsed = format_time(time.time() - t0)\r\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n","\r\n","        # 배치를 GPU에 넣음\r\n","        batch = tuple(t.to(device) for t in batch)\r\n","        \r\n","        # 배치에서 데이터 추출\r\n","        b_input_ids, b_input_mask, b_labels = batch\r\n","\r\n","        # Forward 수행                \r\n","        outputs = model(b_input_ids, \r\n","                        token_type_ids=None, \r\n","                        attention_mask=b_input_mask, \r\n","                        labels=b_labels)\r\n","        \r\n","        # 로스 구함\r\n","        loss = outputs[0]\r\n","\r\n","        # 총 로스 계산\r\n","        total_loss += loss.item()\r\n","\r\n","        # Backward 수행으로 그래디언트 계산\r\n","        loss.backward()\r\n","\r\n","        # 그래디언트 클리핑\r\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n","\r\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\r\n","        optimizer.step()\r\n","\r\n","        # 스케줄러로 학습률 감소\r\n","        scheduler.step()\r\n","\r\n","        # 그래디언트 초기화\r\n","        model.zero_grad()\r\n","\r\n","    # 평균 로스 계산\r\n","    avg_train_loss = total_loss / len(train_dataloader)            \r\n","\r\n","    print(\"\")\r\n","    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\r\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\r\n","        \r\n","    # ========================================\r\n","    #               Validation\r\n","    # ========================================\r\n","\r\n","    print(\"\")\r\n","    print(\"Running Validation...\")\r\n","\r\n","    #시작 시간 설정\r\n","    t0 = time.time()\r\n","\r\n","    # 평가모드로 변경\r\n","    model.eval()\r\n","\r\n","    # 변수 초기화\r\n","    eval_loss, eval_accuracy = 0, 0\r\n","    nb_eval_steps, nb_eval_examples = 0, 0\r\n","\r\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\r\n","    for batch in validation_dataloader:\r\n","        # 배치를 GPU에 넣음\r\n","        batch = tuple(t.to(device) for t in batch)\r\n","        \r\n","        # 배치에서 데이터 추출\r\n","        b_input_ids, b_input_mask, b_labels = batch\r\n","        \r\n","        # 그래디언트 계산 안함\r\n","        with torch.no_grad():     \r\n","            # Forward 수행\r\n","            outputs = model(b_input_ids, \r\n","                            token_type_ids=None, \r\n","                            attention_mask=b_input_mask)\r\n","        \r\n","        # 로스 구함\r\n","        logits = outputs[0]\r\n","\r\n","        # CPU로 데이터 이동\r\n","        logits = logits.detach().cpu().numpy()\r\n","        label_ids = b_labels.to('cpu').numpy()\r\n","        \r\n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\r\n","        tmp_eval_accuracy = calculate_accuracy(logits, label_ids)\r\n","        eval_accuracy += tmp_eval_accuracy\r\n","        nb_eval_steps += 1\r\n","\r\n","    print(\"  Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\r\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\r\n","\r\n","print(\"\")\r\n","print(\"Training complete!\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 3 ========\n","Training...\n","  Batch   500  of  2,969.    Elapsed: 0:01:41.\n","  Batch 1,000  of  2,969.    Elapsed: 0:03:23.\n","  Batch 1,500  of  2,969.    Elapsed: 0:05:04.\n","  Batch 2,000  of  2,969.    Elapsed: 0:06:44.\n","  Batch 2,500  of  2,969.    Elapsed: 0:08:25.\n","\n","  Average training loss: 0.0393\n","  Training epcoh took: 0:09:59\n","\n","Running Validation...\n","  Accuracy: 0.9959\n","  Validation took: 0:00:40\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","  Batch   500  of  2,969.    Elapsed: 0:01:40.\n","  Batch 1,000  of  2,969.    Elapsed: 0:03:20.\n","  Batch 1,500  of  2,969.    Elapsed: 0:05:01.\n","  Batch 2,000  of  2,969.    Elapsed: 0:06:41.\n","  Batch 2,500  of  2,969.    Elapsed: 0:08:21.\n","\n","  Average training loss: 0.0096\n","  Training epcoh took: 0:09:56\n","\n","Running Validation...\n","  Accuracy: 0.9982\n","  Validation took: 0:00:40\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","  Batch   500  of  2,969.    Elapsed: 0:01:40.\n","  Batch 1,000  of  2,969.    Elapsed: 0:03:20.\n","  Batch 1,500  of  2,969.    Elapsed: 0:05:00.\n","  Batch 2,000  of  2,969.    Elapsed: 0:06:40.\n","  Batch 2,500  of  2,969.    Elapsed: 0:08:19.\n","\n","  Average training loss: 0.0024\n","  Training epcoh took: 0:09:53\n","\n","Running Validation...\n","  Accuracy: 0.9981\n","  Validation took: 0:00:40\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dx6dlJXSxh99","executionInfo":{"status":"ok","timestamp":1609257484355,"user_tz":-540,"elapsed":2009651,"user":{"displayName":"‎이혜린(자연과학대학 통계학과)","photoUrl":"","userId":"17465505332261896018"}}},"source":["# 모델 저장\r\n","torch.save(model.state_dict(), './gdrive/My Drive/NH공모전/혜린의 모델찾기대작전/NH_final_model.pt')"],"execution_count":16,"outputs":[]}]}